<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/neutron/doc/source/admin/deploy-lb-ha-vrrp.rst">
    <target refid="deploy-lb-ha-vrrp"></target>
    <section ids="linux-bridge-high-availability-using-vrrp deploy-lb-ha-vrrp" names="linux\ bridge:\ high\ availability\ using\ vrrp deploy-lb-ha-vrrp">
        <title>Linux bridge: High availability using VRRP</title>
        <paragraph>This architecture example augments the self-service deployment example
            with a high-availability mechanism using the Virtual Router Redundancy
            Protocol (VRRP) via <literal>keepalived</literal> and provides failover of routing
            for self-service networks. It requires a minimum of two network nodes
            because VRRP creates one master (active) instance and at least one backup
            instance of each router.</paragraph>
        <paragraph>During normal operation, <literal>keepalived</literal> on the master router periodically
            transmits <emphasis>heartbeat</emphasis> packets over a hidden network that connects all VRRP
            routers for a particular project. Each project with VRRP routers uses a
            separate hidden network. By default this network uses the first value in
            the <literal>tenant_network_types</literal> option in the <literal>ml2_conf.ini</literal> file. For
            additional control, you can specify the self-service network type and physical
            network name for the hidden network using the <literal>l3_ha_network_type</literal> and
            <literal>l3_ha_network_name</literal> options in the <literal>neutron.conf</literal> file.</paragraph>
        <paragraph>If <literal>keepalived</literal> on the backup router stops receiving <emphasis>heartbeat</emphasis> packets,
            it assumes failure of the master router and promotes the backup router to
            master router by configuring IP addresses on the interfaces in the
            <literal>qrouter</literal> namespace. In environments with more than one backup router,
            <literal>keepalived</literal> on the backup router with the next highest priority promotes
            that backup router to master router.</paragraph>
        <note>
            <paragraph>This high-availability mechanism configures VRRP using the same priority
                for all routers. Therefore, VRRP promotes the backup router with the
                highest IP address to the master router.</paragraph>
        </note>
        <warning>
            <paragraph>There is a known bug with <literal>keepalived</literal> v1.2.15 and earlier which can
                cause packet loss when <literal>max_l3_agents_per_router</literal> is set to 3 or more.
                Therefore, we recommend that you upgrade to <literal>keepalived</literal> v1.2.16
                or greater when using this feature.</paragraph>
        </warning>
        <paragraph>Interruption of VRRP <emphasis>heartbeat</emphasis> traffic between network nodes, typically
            due to a network interface or physical network infrastructure failure,
            triggers a failover. Restarting the layer-3 agent, or failure of it, does
            not trigger a failover providing <literal>keepalived</literal> continues to operate.</paragraph>
        <paragraph>Consider the following attributes of this high-availability mechanism to
            determine practicality in your environment:</paragraph>
        <bullet_list bullet="*">
            <list_item>
                <paragraph>Instance network traffic on self-service networks using a particular
                    router only traverses the master instance of that router. Thus,
                    resource limitations of a particular network node can impact all
                    master instances of routers on that network node without triggering
                    failover to another network node. However, you can configure the
                    scheduler to distribute the master instance of each router uniformly
                    across a pool of network nodes to reduce the chance of resource
                    contention on any particular network node.</paragraph>
            </list_item>
            <list_item>
                <paragraph>Only supports self-service networks using a router. Provider networks
                    operate at layer-2 and rely on physical network infrastructure for
                    redundancy.</paragraph>
            </list_item>
            <list_item>
                <paragraph>For instances with a floating IPv4 address, maintains state of network
                    connections during failover as a side effect of 1:1 static NAT. The
                    mechanism does not actually implement connection tracking.</paragraph>
            </list_item>
        </bullet_list>
        <paragraph>For production deployments, we recommend at least three network nodes
            with sufficient resources to handle network traffic for the entire
            environment if one network node fails. Also, the remaining two nodes
            can continue to provide redundancy.</paragraph>
        <warning>
            <paragraph>This high-availability mechanism is not compatible with the layer-2
                population mechanism. You must disable layer-2 population in the
                <literal>linuxbridge_agent.ini</literal> file and restart the Linux bridge agent
                on all existing network and compute nodes prior to deploying the example
                configuration.</paragraph>
        </warning>
        <section ids="prerequisites" names="prerequisites">
            <title>Prerequisites</title>
            <paragraph>Add one network node with the following components:</paragraph>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>Three network interfaces: management, provider, and overlay.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>OpenStack Networking layer-2 agent, layer-3 agent, and any
                        dependencies.</paragraph>
                </list_item>
            </bullet_list>
            <note>
                <paragraph>You can keep the DHCP and metadata agents on each compute node or
                    move them to the network nodes.</paragraph>
            </note>
        </section>
        <section ids="architecture" names="architecture">
            <title>Architecture</title>
            <image alt="High-availability using Linux bridge with VRRP - overview" candidates="{'*': 'admin/figures/deploy-lb-ha-vrrp-overview.png'}" uri="admin/figures/deploy-lb-ha-vrrp-overview.png"></image>
            <paragraph>The following figure shows components and connectivity for one self-service
                network and one untagged (flat) network. The master router resides on network
                node 1. In this particular case, the instance resides on the same compute
                node as the DHCP agent for the network. If the DHCP agent resides on another
                compute node, the latter only contains a DHCP namespace and Linux bridge
                with a port on the overlay physical network interface.</paragraph>
            <image alt="High-availability using Linux bridge with VRRP - components and connectivity - one network" candidates="{'*': 'admin/figures/deploy-lb-ha-vrrp-compconn1.png'}" uri="admin/figures/deploy-lb-ha-vrrp-compconn1.png"></image>
        </section>
        <section ids="example-configuration" names="example\ configuration">
            <title>Example configuration</title>
            <paragraph>Use the following example configuration as a template to add support for
                high-availability using VRRP to an existing operational environment that
                supports self-service networks.</paragraph>
            <section ids="controller-node" names="controller\ node">
                <title>Controller node</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>In the <literal>neutron.conf</literal> file:</paragraph>
                        <bullet_list bullet="*">
                            <list_item>
                                <paragraph>Enable VRRP.</paragraph>
                                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
l3_ha = True</literal_block>
                            </list_item>
                        </bullet_list>
                    </list_item>
                    <list_item>
                        <paragraph>Restart the following services:</paragraph>
                        <bullet_list bullet="*">
                            <list_item>
                                <paragraph>Server</paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="network-node-1" names="network\ node\ 1">
                <title>Network node 1</title>
                <paragraph>No changes.</paragraph>
            </section>
            <section ids="network-node-2" names="network\ node\ 2">
                <title>Network node 2</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Install the Networking service Linux bridge layer-2 agent and layer-3
                            agent.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>In the <literal>neutron.conf</literal> file, configure common options:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
core_plugin = ml2
auth_strategy = keystone

[database]
# ...

[keystone_authtoken]
# ...

[nova]
# ...

[agent]
# ...</literal_block>
                        <paragraph>See the <reference name="Installation Tutorials and Guides" refuri="https://docs.openstack.org">Installation Tutorials and Guides</reference><target ids="installation-tutorials-and-guides" names="installation\ tutorials\ and\ guides" refuri="https://docs.openstack.org"></target> and
                            <reference name="Configuration Reference" refuri="https://docs.openstack.org">Configuration Reference</reference><target ids="configuration-reference" names="configuration\ reference" refuri="https://docs.openstack.org"></target> for your OpenStack
                            release to obtain the appropriate additional configuration for the
                            <literal>[DEFAULT]</literal>, <literal>[database]</literal>, <literal>[keystone_authtoken]</literal>, <literal>[nova]</literal>, and
                            <literal>[agent]</literal> sections.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>In the <literal>linuxbridge_agent.ini</literal> file, configure the layer-2 agent.</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE

[vxlan]
enable_vxlan = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS

[securitygroup]
firewall_driver = iptables</literal_block>
                        <paragraph>Replace <literal>PROVIDER_INTERFACE</literal> with the name of the underlying interface
                            that handles provider networks. For example, <literal>eth1</literal>.</paragraph>
                        <paragraph>Replace <literal>OVERLAY_INTERFACE_IP_ADDRESS</literal> with the IP address of the
                            interface that handles VXLAN overlays for self-service networks.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>In the <literal>l3_agent.ini</literal> file, configure the layer-3 agent.</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
interface_driver = linuxbridge
external_network_bridge =</literal_block>
                        <note>
                            <paragraph>The <literal>external_network_bridge</literal> option intentionally contains
                                no value.</paragraph>
                        </note>
                    </list_item>
                    <list_item>
                        <paragraph>Start the following services:</paragraph>
                        <bullet_list bullet="*">
                            <list_item>
                                <paragraph>Linux bridge agent</paragraph>
                            </list_item>
                            <list_item>
                                <paragraph>Layer-3 agent</paragraph>
                            </list_item>
                        </bullet_list>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="compute-nodes" names="compute\ nodes">
                <title>Compute nodes</title>
                <paragraph>No changes.</paragraph>
            </section>
            <section ids="verify-service-operation" names="verify\ service\ operation">
                <title>Verify service operation</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Source the administrative project credentials.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Verify presence and operation of the agents.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack network agent list
+--------------------------------------+--------------------+----------+-------------------+-------+-------+---------------------------+
| ID                                   | Agent Type         | Host     | Availability Zone | Alive | State | Binary                    |
+--------------------------------------+--------------------+----------+-------------------+-------+-------+---------------------------+
| 09de6af6-c5f1-4548-8b09-18801f068c57 | Linux bridge agent | compute2 |                   | True  | UP    | neutron-linuxbridge-agent |
| 188945d1-9e70-4803-a276-df924e0788a4 | Linux bridge agent | compute1 |                   | True  | UP    | neutron-linuxbridge-agent |
| e76c440d-d5f6-4316-a674-d689630b629e | DHCP agent         | compute1 | nova              | True  | UP    | neutron-dhcp-agent        |
| e67367de-6657-11e6-86a4-931cd04404bb | DHCP agent         | compute2 | nova              | True  | UP    | neutron-dhcp-agent        |
| e8174cae-6657-11e6-89f0-534ac6d0cb5c | Metadata agent     | compute1 |                   | True  | UP    | neutron-metadata-agent    |
| ece49ec6-6657-11e6-bafb-c7560f19197d | Metadata agent     | compute2 |                   | True  | UP    | neutron-metadata-agent    |
| 598f6357-4331-4da5-a420-0f5be000bec9 | L3 agent           | network1 | nova              | True  | UP    | neutron-l3-agent          |
| f4734e0f-bcd5-4922-a19d-e31d56b0a7ae | Linux bridge agent | network1 |                   | True  | UP    | neutron-linuxbridge-agent |
| 670e5805-340b-4182-9825-fa8319c99f23 | Linux bridge agent | network2 |                   | True  | UP    | neutron-linuxbridge-agent |
| 96224e89-7c15-42e9-89c4-8caac7abdd54 | L3 agent           | network2 | nova              | True  | UP    | neutron-l3-agent          |
+--------------------------------------+--------------------+----------+-------------------+-------+-------+---------------------------+</literal_block>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="create-initial-networks" names="create\ initial\ networks">
                <title>Create initial networks</title>
                <paragraph>Similar to the self-service deployment example, this configuration supports
                    multiple VXLAN self-service networks. After enabling high-availability, all
                    additional routers use VRRP. The following procedure creates an additional
                    self-service network and router. The Networking service also supports adding
                    high-availability to existing routers. However, the procedure requires
                    administratively disabling and enabling each router which temporarily
                    interrupts network connectivity for self-service networks with interfaces
                    on that router.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Source a regular (non-administrative) project credentials.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Create a self-service network.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack network create selfservice2
+-------------------------+--------------+
| Field                   | Value        |
+-------------------------+--------------+
| admin_state_up          | UP           |
| mtu                     | 1450         |
| name                    | selfservice2 |
| port_security_enabled   | True         |
| router:external         | Internal     |
| shared                  | False        |
| status                  | ACTIVE       |
+-------------------------+--------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create a IPv4 subnet on the self-service network.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack subnet create --subnet-range 198.51.100.0/24 \
  --network selfservice2 --dns-nameserver 8.8.4.4 selfservice2-v4
+-------------------+------------------------------+
| Field             | Value                        |
+-------------------+------------------------------+
| allocation_pools  | 198.51.100.2-198.51.100.254  |
| cidr              | 198.51.100.0/24              |
| dns_nameservers   | 8.8.4.4                      |
| enable_dhcp       | True                         |
| gateway_ip        | 198.51.100.1                 |
| ip_version        | 4                            |
| name              | selfservice2-v4              |
+-------------------+------------------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create a IPv6 subnet on the self-service network.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack subnet create --subnet-range fd00:198:51:100::/64 --ip-version 6 \
  --ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice2 \
  --dns-nameserver 2001:4860:4860::8844 selfservice2-v6
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | fd00:198:51:100::2-fd00:198:51:100:ffff:ffff:ffff:ffff |
| cidr              | fd00:198:51:100::/64                                   |
| dns_nameservers   | 2001:4860:4860::8844                                   |
| enable_dhcp       | True                                                   |
| gateway_ip        | fd00:198:51:100::1                                     |
| ip_version        | 6                                                      |
| ipv6_address_mode | slaac                                                  |
| ipv6_ra_mode      | slaac                                                  |
| name              | selfservice2-v6                                        |
+-------------------+--------------------------------------------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create a router.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack router create router2
+-----------------------+---------+
| Field                 | Value   |
+-----------------------+---------+
| admin_state_up        | UP      |
| name                  | router2 |
| status                | ACTIVE  |
+-----------------------+---------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Add the IPv4 and IPv6 subnets as interfaces on the router.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack router add subnet router2 selfservice2-v4
$ openstack router add subnet router2 selfservice2-v6</literal_block>
                        <note>
                            <paragraph>These commands provide no output.</paragraph>
                        </note>
                    </list_item>
                    <list_item>
                        <paragraph>Add the provider network as a gateway on the router.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ neutron router-gateway-set router2 provider1
Set gateway for router router2</literal_block>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="verify-network-operation" names="verify\ network\ operation">
                <title>Verify network operation</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Source the administrative project credentials.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Verify creation of the internal high-availability network that handles
                            VRRP <emphasis>heartbeat</emphasis> traffic.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack network list
+--------------------------------------+----------------------------------------------------+--------------------------------------+
| ID                                   | Name                                               | Subnets                              |
+--------------------------------------+----------------------------------------------------+--------------------------------------+
| 1b8519c1-59c4-415c-9da2-a67d53c68455 | HA network tenant f986edf55ae945e2bef3cb4bfd589928 | 6843314a-1e76-4cc9-94f5-c64b7a39364a |
+--------------------------------------+----------------------------------------------------+--------------------------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>On each network node, verify creation of a <literal>qrouter</literal> namespace with
                            the same ID.</paragraph>
                        <paragraph>Network node 1:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96</literal_block>
                        <paragraph>Network node 2:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96</literal_block>
                        <note>
                            <paragraph>The namespace for router 1 from <reference internal="True" refuri="deploy-lb-selfservice#deploy-lb-selfservice"><inline classes="std std-ref">Linux bridge: Self-service networks</inline></reference> should
                                only appear on network node 1 because of creation prior to enabling
                                VRRP.</paragraph>
                        </note>
                    </list_item>
                    <list_item>
                        <paragraph>On each network node, show the IP address of interfaces in the <literal>qrouter</literal>
                            namespace. With the exception of the VRRP interface, only one namespace
                            belonging to the master router instance contains IP addresses on the
                            interfaces.</paragraph>
                        <paragraph>Network node 1:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ha-eb820380-40@if21: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:78:ba:99 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 169.254.192.1/18 brd 169.254.255.255 scope global ha-eb820380-40
       valid_lft forever preferred_lft forever
    inet 169.254.0.1/24 scope global ha-eb820380-40
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe78:ba99/64 scope link
       valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if24: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 198.51.100.1/24 scope global qr-da3504ad-ba
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fedc:8ea8/64 scope link
       valid_lft forever preferred_lft forever
4: qr-442e36eb-fc@if27: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:ee:c8:41 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fd00:198:51:100::1/64 scope global nodad
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:feee:c841/64 scope link
       valid_lft forever preferred_lft forever
5: qg-33fedbc5-43@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 203.0.113.21/24 scope global qg-33fedbc5-43
       valid_lft forever preferred_lft forever
    inet6 fd00:203:0:113::21/64 scope global nodad
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe03:1af6/64 scope link
       valid_lft forever preferred_lft forever</literal_block>
                        <paragraph>Network node 2:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ha-7a7ce184-36@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:16:59:84 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 169.254.192.2/18 brd 169.254.255.255 scope global ha-7a7ce184-36
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe16:5984/64 scope link
       valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
4: qr-442e36eb-fc@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000
5: qg-33fedbc5-43@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0</literal_block>
                        <note>
                            <paragraph>The master router may reside on network node 2.</paragraph>
                        </note>
                    </list_item>
                    <list_item>
                        <paragraph>Launch an instance with an interface on the additional self-service network.
                            For example, a CirrOS image using flavor ID 1.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID selfservice-instance2</literal_block>
                        <paragraph>Replace <literal>NETWORK_ID</literal> with the ID of the additional self-service
                            network.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Determine the IPv4 and IPv6 addresses of the instance.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server list
+--------------------------------------+-----------------------+--------+---------------------------------------------------------------------------+
| ID                                   | Name                  | Status | Networks                                                                  |
+--------------------------------------+-----------------------+--------+---------------------------------------------------------------------------+
| bde64b00-77ae-41b9-b19a-cd8e378d9f8b | selfservice-instance2 | ACTIVE | selfservice2=fd00:198:51:100:f816:3eff:fe71:e93e, 198.51.100.4            |
+--------------------------------------+-----------------------+--------+---------------------------------------------------------------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create a floating IPv4 address on the provider network.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| fixed_ip    | None                                 |
| id          | 0174056a-fa56-4403-b1ea-b5151a31191f |
| instance_id | None                                 |
| ip          | 203.0.113.17                         |
| pool        | provider1                            |
+-------------+--------------------------------------+</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Associate the floating IPv4 address with the instance.</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server add floating ip selfservice-instance2 203.0.113.17</literal_block>
                        <note>
                            <paragraph>This command provides no output.</paragraph>
                        </note>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="verify-failover-operation" names="verify\ failover\ operation">
                <title>Verify failover operation</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Begin a continuous <literal>ping</literal> of both the floating IPv4 address and IPv6
                            address of the instance. While performing the next three steps, you
                            should see a minimal, if any, interruption of connectivity to the
                            instance.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>On the network node with the master router, administratively disable
                            the overlay network interface.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>On the other network node, verify promotion of the backup router to
                            master router by noting addition of IP addresses to the interfaces
                            in the <literal>qrouter</literal> namespace.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>On the original network node in step 2, administratively enable the
                            overlay network interface. Note that the master router remains on
                            the network node in step 3.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="keepalived-vrrp-health-check" names="keepalived\ vrrp\ health\ check">
                <title>Keepalived VRRP health check</title>
                <paragraph>The health of your <literal>keepalived</literal> instances can be automatically monitored via
                    a bash script that verifies connectivity to all available and configured
                    gateway addresses. In the event that connectivity is lost, the master router
                    is rescheduled to another node.</paragraph>
                <paragraph>If all routers lose connectivity simultaneously, the process of selecting a
                    new master router will be repeated in a round-robin fashion until one or more
                    routers have their connectivity restored.</paragraph>
                <paragraph>To enable this feature, edit the <literal>l3_agent.ini</literal> file:</paragraph>
                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">ha_vrrp_health_check_interval = 30</literal_block>
                <paragraph>Where <literal>ha_vrrp_health_check_interval</literal> indicates how often in seconds the
                    health check should run. The default value is <literal>0</literal>, which indicates that the
                    check should not run at all.</paragraph>
            </section>
        </section>
        <section ids="network-traffic-flow" names="network\ traffic\ flow">
            <title>Network traffic flow</title>
            <paragraph>This high-availability mechanism simply augments <reference internal="True" refuri="deploy-lb-selfservice#deploy-lb-selfservice"><inline classes="std std-ref">Linux bridge: Self-service networks</inline></reference>
                with failover of layer-3 services to another router if the master router
                fails. Thus, you can reference <reference internal="True" refuri="deploy-lb-selfservice#deploy-lb-selfservice-networktrafficflow"><inline classes="std std-ref">Self-service network traffic flow</inline></reference> for normal operation.</paragraph>
        </section>
    </section>
</document>
