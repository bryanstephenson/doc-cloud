<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.13.1 -->
<document source="/home/fbaumanis/openstack/soc8_test/openstack_repo/neutron/doc/source/admin/config-sriov.rst">
    <target refid="config-sriov"></target>
    <section ids="sr-iov config-sriov" names="sr-iov config-sriov">
        <title>SR-IOV</title>
        <paragraph>The purpose of this page is to describe how to enable SR-IOV functionality
            available in OpenStack (using OpenStack Networking). This functionality was
            first introduced in the OpenStack Juno release. This page intends to serve as
            a guide for how to configure OpenStack Networking and OpenStack Compute to
            create SR-IOV ports.</paragraph>
        <section ids="the-basics" names="the\ basics">
            <title>The basics</title>
            <paragraph>PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) functionality is
                available in OpenStack since the Juno release. The SR-IOV specification
                defines a standardized mechanism to virtualize PCIe devices. This mechanism
                can virtualize a single PCIe Ethernet controller to appear as multiple PCIe
                devices. Each device can be directly assigned to an instance, bypassing the
                hypervisor and virtual switch layer. As a result, users are able to achieve
                low latency and near-line wire speed.</paragraph>
            <paragraph>The following terms are used throughout this document:</paragraph>
            <table classes="colwidths-given">
                <tgroup cols="2">
                    <colspec colwidth="10"></colspec>
                    <colspec colwidth="90"></colspec>
                    <thead>
                        <row>
                            <entry>
                                <paragraph>Term</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Definition</paragraph>
                            </entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                <paragraph>PF</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Physical Function. The physical Ethernet controller that supports
                                    SR-IOV.</paragraph>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                <paragraph>VF</paragraph>
                            </entry>
                            <entry>
                                <paragraph>Virtual Function. The virtual PCIe device created from a physical
                                    Ethernet controller.</paragraph>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            <section ids="sr-iov-agent" names="sr-iov\ agent">
                <title>SR-IOV agent</title>
                <paragraph>The SR-IOV agent allows you to set the admin state of ports, configure port
                    security (enable and disable spoof checking), and configure QoS rate limiting
                    and minimum bandwidth. You must include the SR-IOV agent on each compute node
                    using SR-IOV ports.</paragraph>
                <note>
                    <paragraph>The SR-IOV agent was optional before Mitaka, and was not enabled by default
                        before Liberty.</paragraph>
                </note>
                <note>
                    <paragraph>The ability to control port security and QoS rate limit settings was added
                        in Liberty.</paragraph>
                </note>
            </section>
            <section ids="supported-ethernet-controllers" names="supported\ ethernet\ controllers">
                <title>Supported Ethernet controllers</title>
                <paragraph>The following manufacturers are known to work:</paragraph>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Intel</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Mellanox</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>QLogic</paragraph>
                    </list_item>
                </bullet_list>
                <paragraph>For information on <strong>Mellanox SR-IOV Ethernet ConnectX-3/ConnectX-3 Pro cards</strong>, see
                    <reference name="Mellanox: How To Configure SR-IOV VFs" refuri="https://community.mellanox.com/docs/DOC-1484">Mellanox: How To Configure SR-IOV VFs</reference><target ids="mellanox-how-to-configure-sr-iov-vfs" names="mellanox:\ how\ to\ configure\ sr-iov\ vfs" refuri="https://community.mellanox.com/docs/DOC-1484"></target>.</paragraph>
                <paragraph>For information on <strong>QLogic SR-IOV Ethernet cards</strong>, see
                    <reference name="User's Guide OpenStack Deployment with SR-IOV Configuration" refuri="http://www.qlogic.com/solutions/Documents/UsersGuide_OpenStack_SR-IOV.pdf">Userâ€™s Guide OpenStack Deployment with SR-IOV Configuration</reference><target ids="user-s-guide-openstack-deployment-with-sr-iov-configuration" names="user's\ guide\ openstack\ deployment\ with\ sr-iov\ configuration" refuri="http://www.qlogic.com/solutions/Documents/UsersGuide_OpenStack_SR-IOV.pdf"></target>.</paragraph>
            </section>
        </section>
        <section ids="using-sr-iov-interfaces" names="using\ sr-iov\ interfaces">
            <title>Using SR-IOV interfaces</title>
            <paragraph>In order to enable SR-IOV, the following steps are required:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Create Virtual Functions (Compute)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Whitelist PCI devices in nova-compute (Compute)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Configure neutron-server (Controller)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Configure nova-scheduler (Controller)</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Enable neutron sriov-agent (Compute)</paragraph>
                </list_item>
            </enumerated_list>
            <paragraph>We recommend using VLAN provider networks for segregation. This way you can
                combine instances without SR-IOV ports and instances with SR-IOV ports on a
                single network.</paragraph>
            <note>
                <paragraph>Throughout this guide, <literal>eth3</literal> is used as the PF and <literal>physnet2</literal> is used
                    as the provider network configured as a VLAN range. These ports may vary in
                    different environments.</paragraph>
            </note>
            <section ids="create-virtual-functions-compute" names="create\ virtual\ functions\ (compute)">
                <title>Create Virtual Functions (Compute)</title>
                <paragraph>Create the VFs for the network interface that will be used for SR-IOV. We use
                    <literal>eth3</literal> as PF, which is also used as the interface for the VLAN provider
                    network and has access to the private networks of all machines.</paragraph>
                <note>
                    <paragraph>The steps detail how to create VFs using Mellanox ConnectX-4 and newer/Intel
                        SR-IOV Ethernet cards on an Intel system. Steps may differ for different
                        hardware configurations.</paragraph>
                </note>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Ensure SR-IOV and VT-d are enabled in BIOS.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Enable IOMMU in Linux by adding <literal>intel_iommu=on</literal> to the kernel parameters,
                            for example, using GRUB.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>On each compute node, create the VFs via the PCI SYS interface:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># echo '8' &gt; /sys/class/net/eth3/device/sriov_numvfs</literal_block>
                        <note>
                            <paragraph>On some PCI devices, observe that when changing the amount of VFs you
                                receive the error <literal>Device or resource busy</literal>. In this case, you must
                                first set <literal>sriov_numvfs</literal> to <literal>0</literal>, then set it to your new value.</paragraph>
                        </note>
                        <note>
                            <paragraph>A network interface could be used both for PCI passthrough, using the PF,
                                and SR-IOV, using the VFs. If the PF is used, the VF number stored in
                                the <literal>sriov_numvfs</literal> file is lost. If the PF is attached again to the
                                operating system, the number of VFs assigned to this interface will be
                                zero. To keep the number of VFs always assigned to this interface,
                                modify the interfaces configuration file adding an <literal>ifup</literal> script
                                command.</paragraph>
                            <paragraph>In Ubuntu, modifying the <literal>/etc/network/interfaces</literal> file:</paragraph>
                            <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">auto eth3
iface eth3 inet dhcp
pre-up echo '4' &gt; /sys/class/net/eth3/device/sriov_numvfs</literal_block>
                            <paragraph>In Red Hat, modifying the <literal>/sbin/ifup-local</literal> file:</paragraph>
                            <literal_block highlight_args="{}" language="bash" linenos="False" xml:space="preserve">#!/bin/sh
if [[ "$1" == "eth3" ]]
then
    echo '4' &gt; /sys/class/net/eth3/device/sriov_numvfs
fi</literal_block>
                        </note>
                        <warning>
                            <paragraph>Alternatively, you can create VFs by passing the <literal>max_vfs</literal> to the
                                kernel module of your network interface. However, the <literal>max_vfs</literal>
                                parameter has been deprecated, so the PCI SYS interface is the preferred
                                method.</paragraph>
                        </warning>
                        <paragraph>You can determine the maximum number of VFs a PF can support:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># cat /sys/class/net/eth3/device/sriov_totalvfs
63</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Verify that the VFs have been created and are in <literal>up</literal> state:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># lspci | grep Ethernet
82:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
82:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
82:10.0 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:10.2 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:10.4 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:10.6 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:11.0 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:11.2 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:11.4 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
82:11.6 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)</literal_block>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip link show eth3
8: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT qlen 1000
   link/ether a0:36:9f:8f:3f:b8 brd ff:ff:ff:ff:ff:ff
   vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 4 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 5 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 6 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 7 MAC 00:00:00:00:00:00, spoof checking on, link-state auto</literal_block>
                        <paragraph>If the interfaces are down, set them to <literal>up</literal> before launching a guest,
                            otherwise the instance will fail to spawn:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># ip link set eth3 up</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Persist created VFs on reboot:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># echo "echo '7' &gt; /sys/class/net/eth3/device/sriov_numvfs" &gt;&gt; /etc/rc.local</literal_block>
                        <note>
                            <paragraph>The suggested way of making PCI SYS settings persistent is through
                                the <literal>sysfsutils</literal> tool. However, this is not available by default on
                                many major distributions.</paragraph>
                        </note>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="whitelist-pci-devices-nova-compute-compute" names="whitelist\ pci\ devices\ nova-compute\ (compute)">
                <title>Whitelist PCI devices nova-compute (Compute)</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Configure which PCI devices the <literal>nova-compute</literal> service may use. Edit
                            the <literal>nova.conf</literal> file:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[default]
pci_passthrough_whitelist = { "devname": "eth3", "physical_network": "physnet2"}</literal_block>
                        <paragraph>This tells the Compute service that all VFs belonging to <literal>eth3</literal> are
                            allowed to be passed through to instances and belong to the provider network
                            <literal>physnet2</literal>.</paragraph>
                        <paragraph>Alternatively the <literal>pci_passthrough_whitelist</literal> parameter also supports
                            whitelisting by:</paragraph>
                        <bullet_list bullet="-">
                            <list_item>
                                <paragraph>PCI address: The address uses the same syntax as in <literal>lspci</literal> and an
                                    asterisk (*) can be used to match anything.</paragraph>
                                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">pci_passthrough_whitelist = { "address": "[[[[&lt;domain&gt;]:]&lt;bus&gt;]:][&lt;slot&gt;][.[&lt;function&gt;]]", "physical_network": "physnet2" }</literal_block>
                                <paragraph>For example, to match any domain, bus 0a, slot 00, and all functions:</paragraph>
                                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">pci_passthrough_whitelist = { "address": "*:0a:00.*", "physical_network": "physnet2" }</literal_block>
                            </list_item>
                            <list_item>
                                <paragraph>PCI <literal>vendor_id</literal> and <literal>product_id</literal> as displayed by the Linux utility
                                    <literal>lspci</literal>.</paragraph>
                                <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">pci_passthrough_whitelist = { "vendor_id": "&lt;id&gt;", "product_id": "&lt;id&gt;", "physical_network": "physnet2" }</literal_block>
                            </list_item>
                        </bullet_list>
                        <paragraph>If the device defined by the PCI address or <literal>devname</literal> corresponds to an
                            SR-IOV PF, all VFs under the PF will match the entry. Multiple
                            <literal>pci_passthrough_whitelist</literal> entries per host are supported.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Restart the <literal>nova-compute</literal> service for the changes to go into effect.</paragraph>
                    </list_item>
                </enumerated_list>
                <target refid="configure-sriov-neutron-server"></target>
            </section>
            <section ids="configure-neutron-server-controller configure-sriov-neutron-server" names="configure\ neutron-server\ (controller) configure_sriov_neutron_server">
                <title>Configure neutron-server (Controller)</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Add <literal>sriovnicswitch</literal> as mechanism driver. Edit the <literal>ml2_conf.ini</literal> file
                            on each controller:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">mechanism_drivers = openvswitch,sriovnicswitch</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Add the <literal>plugin.ini</literal> file as a parameter to the <literal>neutron-server</literal>
                            service. Edit the appropriate initialization script to configure the
                            <literal>neutron-server</literal> service to load the plugin configuration file:</paragraph>
                        <literal_block highlight_args="{}" language="bash" linenos="False" xml:space="preserve">--config-file /etc/neutron/neutron.conf
--config-file /etc/neutron/plugin.ini</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Restart the <literal>neutron-server</literal> service.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="configure-nova-scheduler-controller" names="configure\ nova-scheduler\ (controller)">
                <title>Configure nova-scheduler (Controller)</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>On every controller node running the <literal>nova-scheduler</literal> service, add
                            <literal>PciPassthroughFilter</literal> to <literal>scheduler_default_filters</literal> to enable
                            <literal>PciPassthroughFilter</literal> by default.
                            Also ensure <literal>scheduler_available_filters</literal> parameter under the
                            <literal>[DEFAULT]</literal> section in <literal>nova.conf</literal> is set to <literal>all_filters</literal>
                            to enable all filters provided by the Compute service.</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[DEFAULT]
scheduler_default_filters = RetryFilter, AvailabilityZoneFilter, RamFilter, ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter, PciPassthroughFilter
scheduler_available_filters = nova.scheduler.filters.all_filters</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Restart the <literal>nova-scheduler</literal> service.</paragraph>
                    </list_item>
                </enumerated_list>
            </section>
            <section ids="enable-neutron-sriov-agent-compute" names="enable\ neutron\ sriov-agent\ (compute)">
                <title>Enable neutron sriov-agent (Compute)</title>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Install the SR-IOV agent.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Edit the <literal>sriov_agent.ini</literal> file on each compute node. For example:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver

[sriov_nic]
physical_device_mappings = physnet2:eth3
exclude_devices =</literal_block>
                        <note>
                            <paragraph>The <literal>physical_device_mappings</literal> parameter is not limited to be a 1-1
                                mapping between physical networks and NICs. This enables you to map the
                                same physical network to more than one NIC. For example, if <literal>physnet2</literal>
                                is connected to <literal>eth3</literal> and <literal>eth4</literal>, then
                                <literal>physnet2:eth3,physnet2:eth4</literal> is a valid option.</paragraph>
                        </note>
                        <paragraph>The <literal>exclude_devices</literal> parameter is empty, therefore, all the VFs
                            associated with eth3 may be configured by the agent. To exclude specific
                            VFs, add them to the <literal>exclude_devices</literal> parameter as follows:</paragraph>
                        <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">exclude_devices = eth1:0000:07:00.2;0000:07:00.3,eth2:0000:05:00.1;0000:05:00.2</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Ensure the neutron sriov-agent runs successfully:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve"># neutron-sriov-nic-agent \
  --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/sriov_agent.ini</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Enable the neutron sriov-agent service.</paragraph>
                        <paragraph>If installing from source, you must configure a daemon file for the init
                            system manually.</paragraph>
                    </list_item>
                </enumerated_list>
                <section ids="optional-fdb-l2-agent-extension" names="(optional)\ fdb\ l2\ agent\ extension">
                    <title>(Optional) FDB L2 agent extension</title>
                    <paragraph>Forwarding DataBase (FDB) population is an L2 agent extension to OVS agent or
                        Linux bridge. Its objective is to update the FDB table for existing instance
                        using normal port. This enables communication between SR-IOV instances and
                        normal instances. The use cases of the FDB population extension are:</paragraph>
                    <bullet_list bullet="*">
                        <list_item>
                            <paragraph>Direct port and normal port instances reside on the same compute node.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Direct port instance that uses floating IP address and network node
                                are located on the same host.</paragraph>
                        </list_item>
                    </bullet_list>
                    <paragraph>For additional information describing the problem, refer to:
                        <reference name="Virtual switching technologies and Linux bridge." refuri="http://events.linuxfoundation.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdf">Virtual switching technologies and Linux bridge.</reference><target ids="virtual-switching-technologies-and-linux-bridge" names="virtual\ switching\ technologies\ and\ linux\ bridge." refuri="http://events.linuxfoundation.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdf"></target></paragraph>
                    <enumerated_list enumtype="arabic" prefix="" suffix=".">
                        <list_item>
                            <paragraph>Edit the <literal>ovs_agent.ini</literal> or <literal>linuxbridge_agent.ini</literal> file on each compute
                                node. For example:</paragraph>
                            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">[agent]
extensions = fdb</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Add the FDB section and the <literal>shared_physical_device_mappings</literal> parameter.
                                This parameter maps each physical port to its physical network name. Each
                                physical network can be mapped to several ports:</paragraph>
                            <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">[FDB]
shared_physical_device_mappings = physnet1:p1p1, physnet1:p1p2</literal_block>
                        </list_item>
                    </enumerated_list>
                </section>
            </section>
            <section ids="launching-instances-with-sr-iov-ports" names="launching\ instances\ with\ sr-iov\ ports">
                <title>Launching instances with SR-IOV ports</title>
                <paragraph>Once configuration is complete, you can launch instances with SR-IOV ports.</paragraph>
                <enumerated_list enumtype="arabic" prefix="" suffix=".">
                    <list_item>
                        <paragraph>Get the <literal>id</literal> of the network where you want the SR-IOV port to be created:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ net_id=`neutron net-show net04 | grep "\ id\ " | awk '{ print $4 }'`</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create the SR-IOV port. <literal>vnic_type=direct</literal> is used here, but other options
                            include <literal>normal</literal>, <literal>direct-physical</literal>, and <literal>macvtap</literal>:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ port_id=`neutron port-create $net_id --name sriov_port --binding:vnic_type direct | grep "\ id\ " | awk '{ print $4 }'`</literal_block>
                    </list_item>
                    <list_item>
                        <paragraph>Create the instance. Specify the SR-IOV port created in step two for the
                            NIC:</paragraph>
                        <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ openstack server create --flavor m1.large --image ubuntu_14.04 --nic port-id=$port_id test-sriov</literal_block>
                        <note>
                            <paragraph>There are two ways to attach VFs to an instance. You can create an SR-IOV
                                port or use the <literal>pci_alias</literal> in the Compute service. For more
                                information about using <literal>pci_alias</literal>, refer to <reference name="nova-api configuration" refuri="https://docs.openstack.org/admin-guide/compute-pci-passthrough.html#configure-nova-api-controller">nova-api configuration</reference>.</paragraph>
                        </note>
                    </list_item>
                </enumerated_list>
            </section>
        </section>
        <section ids="sr-iov-with-infiniband" names="sr-iov\ with\ infiniband">
            <title>SR-IOV with InfiniBand</title>
            <paragraph>The support for SR-IOV with InfiniBand allows a Virtual PCI device (VF) to
                be directly mapped to the guest, allowing higher performance and advanced
                features such as RDMA (remote direct memory access). To use this feature,
                you must:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>Use InfiniBand enabled network adapters.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Run InfiniBand subnet managers to enable InfiniBand fabric.</paragraph>
                    <paragraph>All InfiniBand networks must have a subnet manager running for the network
                        to function. This is true even when doing a simple network of two
                        machines with no switch and the cards are plugged in back-to-back. A
                        subnet manager is required for the link on the cards to come up.
                        It is possible to have more than one subnet manager. In this case, one
                        of them will act as the master, and any other will act as a slave that
                        will take over when the master subnet manager fails.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Install the <literal>ebrctl</literal> utility on the compute nodes.</paragraph>
                    <paragraph>Check that <literal>ebrctl</literal> is listed somewhere in <literal>/etc/nova/rootwrap.d/*</literal>:</paragraph>
                    <literal_block highlight_args="{}" language="console" linenos="False" xml:space="preserve">$ grep 'ebrctl' /etc/nova/rootwrap.d/*</literal_block>
                    <paragraph>If <literal>ebrctl</literal> does not appear in any of the rootwrap files, add this to the
                        <literal>/etc/nova/rootwrap.d/compute.filters</literal> file in the <literal>[Filters]</literal> section.</paragraph>
                    <literal_block highlight_args="{}" language="none" linenos="False" xml:space="preserve">[Filters]
ebrctl: CommandFilter, ebrctl, root</literal_block>
                </list_item>
            </enumerated_list>
        </section>
        <section ids="known-limitations" names="known\ limitations">
            <title>Known limitations</title>
            <bullet_list bullet="*">
                <list_item>
                    <paragraph>When using Quality of Service (QoS), <literal>max_burst_kbps</literal> (burst over
                        <literal>max_kbps</literal>) is not supported. In addition, <literal>max_kbps</literal> is rounded to
                        Mbps.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Security groups are not supported when using SR-IOV, thus, the firewall
                        driver must be disabled. This can be done in the <literal>neutron.conf</literal> file.</paragraph>
                    <literal_block highlight_args="{}" language="ini" linenos="False" xml:space="preserve">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver</literal_block>
                </list_item>
                <list_item>
                    <paragraph>SR-IOV is not integrated into the OpenStack Dashboard (horizon). Users must
                        use the CLI or API to configure SR-IOV interfaces.</paragraph>
                </list_item>
                <list_item>
                    <paragraph>Live migration is not supported for instances with SR-IOV ports.</paragraph>
                    <note>
                        <paragraph>SR-IOV features may require a specific NIC driver version, depending on the vendor.
                            Intel NICs, for example, require ixgbe version 4.4.6 or greater, and ixgbevf version
                            3.2.2 or greater.</paragraph>
                    </note>
                </list_item>
            </bullet_list>
        </section>
    </section>
</document>
