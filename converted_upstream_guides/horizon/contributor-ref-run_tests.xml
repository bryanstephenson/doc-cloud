<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
  <title>The run_tests.sh Script</title>
  <warning>
    <para>This script is deprecated as of Newton (11.0), and will be removed in
                Queens (13.0), in favor of tox. The tox docs can be found at
                <link xlink:href="https://tox.readthedocs.io/en/latest/"/></para>
  </warning>
  <para>Horizon ships with a script called <literal>run_tests.sh</literal> at the root of the
            repository. This script provides many crucial functions for the project,
            and also makes several otherwise complex tasks trivial for you as a
            developer.</para>
  <section>
    <title>First Run</title>
    <para>If you start with a clean copy of the Horizon repository, the first thing
                you should do is to run <literal>./run_tests.sh</literal> from the root of the repository.
                This will do two things for you:</para>
    <procedure>
      <step>
        <para>Set up a virtual environment for both the <literal>horizon</literal> module and
                        the <literal>openstack_dashboard</literal> project using <literal>./tools/install_venv.py</literal>.</para>
      </step>
      <step>
        <para>Run the tests for both <literal>horizon</literal> and <literal>openstack_dashboard</literal> using
                        their respective environments and verify that everything is working.</para>
      </step>
    </procedure>
    <para>Setting up the environment the first time can take several minutes, but only
                needs to be done once. If dependencies are added in the future, updating the
                environments will be necessary but not as time consuming.</para>
  </section>
  <section>
    <title>I just want to run the tests!</title>
    <para>Running the full set of unit tests quickly and easily is the main goal of this
                script. All you need to do is:</para>
    <screen>./run_tests.sh</screen>
    <para>Yep, that’s it. However, for a more thorough test run you can include the
                Selenium tests by using the <literal>--with-selenium</literal> flag:</para>
    <screen>./run_tests.sh --with-selenium</screen>
    <para>If you run horizon in a minimal installation VM, you will probably need
                the following (steps for Fedora 18 minimal installation):</para>
    <procedure>
      <step>
        <para>Install these packages in the VM:
                        <literal>yum install xorg-x11-xauth xorg-x11-fonts-Type1.noarch</literal>.</para>
      </step>
      <step>
        <para>Install firefox in the VM:
                        <literal>yum install firefox</literal>.</para>
      </step>
      <step>
        <para>Connect to the VM by <literal>ssh -X</literal>
                        (if you run <literal>set|grep DISP</literal>, you should see that the DISPLAY is set).</para>
      </step>
      <step>
        <para>Run <literal>./run_tests.sh --with-selenium</literal>.</para>
      </step>
    </procedure>
    <section>
      <title>Running a subset of tests</title>
      <para>Instead of running all tests, you can specify an individual directory, file,
                    class, or method that contains test code.</para>
      <para>To run the tests in the <literal>horizon/test/tests/tables.py</literal> file:</para>
      <screen>./run_tests.sh horizon.test.tests.tables</screen>
      <para>To run the tests in the <literal>WorkflowsTests</literal> class in
                    <literal>horizon/test/tests/workflows</literal>:</para>
      <screen>./run_tests.sh horizon.test.tests.workflows:WorkflowsTests</screen>
      <para>To run just the <literal>WorkflowsTests.test_workflow_view</literal> test method:</para>
      <screen>./run_tests.sh horizon.test.tests.workflows:WorkflowsTests.test_workflow_view</screen>
    </section>
    <section>
      <title>Running the integration tests</title>
      <para>The Horizon integration tests treat Horizon as a black box, and similar
                    to Tempest must be run against an existing OpenStack system. These
                    tests are not run by default.</para>
      <procedure>
        <step>
          <para>Update the configuration file
                            <literal>openstack_dashboard/test/integration_tests/horizon.conf</literal> as
                            required (the format is similar to the Tempest configuration file).</para>
        </step>
        <step>
          <para>Run the tests with the following command:</para>
          <screen>$ ./run_tests.sh --integration</screen>
        </step>
      </procedure>
      <para>Like for the unit tests, you can choose to only run a subset.</para>
      <screen><?dbsuse-fo font-size="8pt"?>$ ./run_tests.sh --integration openstack_dashboard.test.integration_tests.tests.test_login</screen>
    </section>
  </section>
  <section>
    <title>Using Dashboard and Panel Templates</title>
    <para>Horizon has a set of convenient management commands for creating new
                dashboards and panels based on basic templates.</para>
    <section>
      <title>Dashboards</title>
      <para>To create a new dashboard, run the following:</para>
      <screen>./run_tests.sh -m startdash &lt;dash_name&gt;</screen>
      <para>This will create a directory with the given dashboard name, a <literal>dashboard.py</literal>
                    module with the basic dashboard code filled in, and various other common
                    “boilerplate” code.</para>
      <para>Available options:</para>
      <itemizedlist>
        <listitem>
          <para><literal>--target</literal>: the directory in which the dashboard files should be created.
                            Default: A new directory within the current directory.</para>
        </listitem>
      </itemizedlist>
    </section>
    <section>
      <title>Panels</title>
      <para>To create a new panel, run the following:</para>
      <screen>./run_tests -m startpanel &lt;panel_name&gt;</screen>
      <para>This will create a directory with the given panel name, and <literal>panel.py</literal>
                    module with the basic panel code filled in, and various other common
                    “boilerplate” code.</para>
      <para>Available options:</para>
      <itemizedlist>
        <listitem>
          <para><literal>-d</literal>, <literal>--dashboard</literal>: The dotted python path to your dashboard app (the
                            module which contains the <literal>dashboard.py</literal> file.). If not specified, the
                            target dashboard should be specified in a pluggable settings file for the
                            panel.</para>
        </listitem>
        <listitem>
          <para><literal>--target</literal>: the directory in which the panel files should be created.
                            If the value is <literal>auto</literal> the panel will be created as a new directory inside
                            the dashboard module’s directory structure. Default: A new directory within
                            the current directory.</para>
        </listitem>
      </itemizedlist>
    </section>
    <section>
      <title>JavaScript Tests</title>
      <para>You can also run JavaScript unit tests using Karma.  Karma is a test
                    environment that allows for multiple test runners and reporters, including
                    such features as code coverage.  Karma allows developer to run tests live,
                    as it can watch source and test files for changes.</para>
      <para>The default configuration also performs coverage reports, which are saved
                    to <literal>cover/horizon/</literal> and <literal>cover/openstack_dashboard/</literal>.</para>
      <para>To run the Karma tests for Horizon and Dashboard using the <literal>run_tests.sh</literal>
                    script:</para>
      <screen>./run_tests.sh --karma</screen>
      <para>To run the Karma tests for Horizon and Dashboard using <literal>npm</literal>:</para>
      <screen>npm install # You only need to execute this once.
npm test</screen>
      <note>
        <para>These two methods are equivalent. The former merely executes
                        the latter.</para>
      </note>
    </section>
    <section>
      <title>JavaScript Code Style Checks</title>
      <para>You can run the JavaScript code style checks, or linting, using eslint.
                    ESLint is a permissively licensed, sophisticated language parser and
                    linter that confirms both our style guidelines, and checks the code for
                    common errors that may create unexpected behavior.</para>
      <para>To run eslint for Horizon and Dashboard using the <literal>run_tests.sh</literal>
                    script:</para>
      <screen>./run_tests.sh --eslint</screen>
      <para>To run eslint for Horizon and Dashboard using <literal>npm</literal>:</para>
      <screen>npm install # You only need to execute this once.
npm run lint</screen>
      <note>
        <para>These two methods are equivalent. The former merely executes
                        the latter.</para>
      </note>
    </section>
  </section>
  <section>
    <title>Give me metrics!</title>
    <para>You can generate various reports and metrics using command line arguments
                to <literal>run_tests.sh</literal>.</para>
    <section>
      <title>ESLint</title>
      <para>To run ESLint, a JavaScript code style checker:</para>
      <screen>./run_tests.sh --eslint</screen>
    </section>
    <section>
      <title>Coverage</title>
      <para>To run coverage reports:</para>
      <screen>./run_tests.sh --coverage</screen>
      <para>The reports are saved to <literal>./reports/</literal> and <literal>./coverage.xml</literal>.</para>
    </section>
    <section>
      <title>PEP8</title>
      <para>You can check for PEP8 violations as well:</para>
      <screen>./run_tests.sh --pep8</screen>
      <para>The results are saved to <literal>./pep8.txt</literal>.</para>
    </section>
    <section>
      <title>PyLint</title>
      <para>For more detailed code analysis you can run:</para>
      <screen>./run_tests.sh --pylint</screen>
      <para>The output will be saved in <literal>./pylint.txt</literal>.</para>
    </section>
    <section>
      <title>Tab Characters</title>
      <para>For those who dislike having a mix of tab characters and spaces for indentation
                    there’s a command to check for that in Python, CSS, JavaScript and HTML files:</para>
      <screen>./run_tests.sh --tabs</screen>
      <para>This will output a total “tab count” and a list of the offending files.</para>
    </section>
  </section>
  <section>
    <title>Running the development server</title>
    <para>As an added bonus, you can run Django’s development server directly from
                the root of the repository with <literal>run_tests.sh</literal> like so:</para>
    <screen>./run_tests.sh --runserver</screen>
    <para>This is effectively just an alias for:</para>
    <screen>./tools/with_venv.sh ./manage.py runserver</screen>
  </section>
  <section>
    <title>Generating the documentation</title>
    <para>You can build Horizon’s documentation automatically by running:</para>
    <screen>./run_tests.sh --docs</screen>
    <para>The output is stored in <literal>./doc/build/html/</literal>.</para>
  </section>
  <section>
    <title>Updating the translation files</title>
    <para>You can update all of the translation files for both the <literal>horizon</literal> app and
                <literal>openstack_dashboard</literal> project with a single command:</para>
    <screen>./run_tests.sh --makemessages</screen>
    <para>or, more compactly:</para>
    <screen>./run_tests.sh --m</screen>
  </section>
  <section>
    <title>Starting clean</title>
    <para>If you ever want to start clean with a new environment for Horizon, you can
                run:</para>
    <screen>./run_tests.sh --force</screen>
    <para>That will blow away the existing environments and create new ones for you.</para>
  </section>
  <section>
    <title>Non-interactive Mode</title>
    <para>There is an optional flag which will run the script in a non-interactive
                (and eventually less verbose) mode:</para>
    <screen>./run_tests.sh --quiet</screen>
    <para>This will automatically take the default action for actions which would
                normally prompt for user input such as installing/updating the environment.</para>
  </section>
  <section>
    <title>Environment Backups</title>
    <para>To speed up the process of doing clean checkouts, running continuous
                integration tests, etc. there are options for backing up the current
                environment and restoring from a backup:</para>
    <screen>./run_tests.sh --restore-environment
./run_tests.sh --backup-environment</screen>
    <para>The environment backup is stored in <literal>/tmp/.horizon_environment/</literal>.</para>
  </section>
  <section>
    <title>Environment Versioning</title>
    <para>Horizon keeps track of changes to the environment by comparing the
                current requirements files (<literal>requirements.txt</literal> and
                <literal>test-requirements.txt</literal>) and the files last time the virtual
                environment was created or updated. If there is any difference,
                the virtual environment will be update automatically when you run
                <literal>run_tests.sh</literal>.</para>
  </section>
</section>
